// THIS FILE IS AUTOMATICALLY GENERATED: DO NOT EDIT

= image:kamelets/aws-s3-source.svg[] AWS S3 Source

*Provided by: "Red Hat"*

Receive data from an Amazon S3 Bucket.

The basic authentication method for the S3 service is to specify an access key and a secret key. These parameters are optional because the Kamelet provides a default credentials provider.

If you use the default credentials provider, the S3 client loads the credentials through this provider and doesn't use the basic authentication method.

Two headers will be duplicated with different names for clarity at sink level, CamelAwsS3Key will be duplicated into aws.s3.key and CamelAwsS3BucketName will be duplicated in aws.s3.bucket.name

== Configuration Options

The following table summarizes the configuration options available for the `aws-s3-source` Kamelet:
[width="100%",cols="2,^2,3,^2,^2,^3",options="header"]
|===
| Property| Name| Description| Type| Default| Example
| *bucketNameOrArn {empty}* *| Bucket Name| The S3 Bucket name or Amazon Resource Name (ARN).| string| | 
| *region {empty}* *| AWS Region| The AWS region to access.| string| | 
| accessKey| Access Key| The access key obtained from AWS.| string| | 
| autoCreateBucket| Autocreate Bucket| Specifies to automatically create the S3 bucket.| boolean| `false`| 
| delay| Delay| The number of milliseconds before the next poll of the selected bucket.| integer| `500`| 
| deleteAfterRead| Auto-delete Objects| Specifies to delete objects after consuming them.| boolean| `true`| 
| ignoreBody| Ignore Body| If true, the S3 Object body is ignored. Setting this to true overrides any behavior defined by the `includeBody` option. If false, the S3 object is put in the body.| boolean| `false`| 
| maxMessagesPerPoll| Max Messages Per Poll| Gets the maximum number of messages as a limit to poll at each polling. Gets the maximum number of messages as a limit to poll at each polling. The default value is 10. Use 0 or a negative number to set it as unlimited.| integer| `10`| 
| overrideEndpoint| Endpoint Overwrite| Select this option to override the endpoint URI. To use this option, you must also provide a URI for the `uriEndpointOverride` option.| boolean| `false`| 
| prefix| Prefix| The AWS S3 bucket prefix to consider while searching.| string| | `"folder/"`
| secretKey| Secret Key| The secret key obtained from AWS.| string| | 
| uriEndpointOverride| Overwrite Endpoint URI| The overriding endpoint URI. To use this option, you must also select the `overrideEndpoint` option.| string| | 
| useDefaultCredentialsProvider| Default Credentials Provider| If true, the S3 client loads credentials through a default credentials provider. If false, it uses the basic authentication method (access key and secret key).| boolean| `false`| 
|===

NOTE: Fields marked with an asterisk ({empty}*) are mandatory.


== Dependencies

At runtime, the `aws-s3-source` Kamelet relies upon the presence of the following dependencies:

- camel:core
- camel:aws2-s3
- github:openshift-integration.kamelet-catalog:camel-kamelets-utils:2.2.0-SNAPSHOT
- camel:kamelet 

== Usage

This section describes how you can use the `aws-s3-source`.

=== Knative Source

You can use the `aws-s3-source` Kamelet as a Knative source by binding it to a Knative object.

.aws-s3-source-binding.yaml
[source,yaml]
----
apiVersion: camel.apache.org/v1
kind: KameletBinding
metadata:
  name: aws-s3-source-binding
spec:
  source:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: aws-s3-source
    properties:
      bucketNameOrArn: "The Bucket Name"
      region: "The AWS Region"
  sink:
    ref:
      kind: Channel
      apiVersion: messaging.knative.dev/v1
      name: mychannel
  
----

==== *Prerequisite*

Make sure you have *"Red Hat Integration - Camel K"* installed into the OpenShift cluster you're connected to.

==== *Procedure for using the cluster CLI*

. Save the `aws-s3-source-binding.yaml` file to your local drive, and then edit it as needed for your configuration.

. Run the source by using the following command:
+
[source,shell]
----
oc apply -f aws-s3-source-binding.yaml
----

==== *Procedure for using the Kamel CLI*

Configure and run the source by using the following command:

[source,shell]
----
kamel bind aws-s3-source -p "source.bucketNameOrArn=The Bucket Name" -p "source.region=The AWS Region" channel:mychannel
----

This command creates the KameletBinding in the current namespace on the cluster.

=== Kafka Source

You can use the `aws-s3-source` Kamelet as a Kafka source by binding it to a Kafka topic.

.aws-s3-source-binding.yaml
[source,yaml]
----
apiVersion: camel.apache.org/v1
kind: KameletBinding
metadata:
  name: aws-s3-source-binding
spec:
  source:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: aws-s3-source
    properties:
      bucketNameOrArn: "The Bucket Name"
      region: "The AWS Region"
  sink:
    ref:
      kind: KafkaTopic
      apiVersion: kafka.strimzi.io/v1beta1
      name: my-topic
  
----

==== *Prerequisites*

Ensure that you've installed the *AMQ Streams* operator in your OpenShift cluster and created a topic named `my-topic` in the current namespace.
Make also sure you have *"Red Hat Integration - Camel K"* installed into the OpenShift cluster you're connected to.

==== *Procedure for using the cluster CLI*

. Save the `aws-s3-source-binding.yaml` file to your local drive, and then edit it as needed for your configuration.

. Run the source by using the following command:
+
[source,shell]
----
oc apply -f aws-s3-source-binding.yaml
----

==== *Procedure for using the Kamel CLI*

Configure and run the source by using the following command:

[source,shell]
----
kamel bind aws-s3-source -p "source.bucketNameOrArn=The Bucket Name" -p "source.region=The AWS Region" kafka.strimzi.io/v1beta1:KafkaTopic:my-topic
----

This command creates the KameletBinding in the current namespace on the cluster.

== Kamelet source file

https://github.com/openshift-integration/kamelet-catalog/blob/main/aws-s3-source.kamelet.yaml

// THIS FILE IS AUTOMATICALLY GENERATED: DO NOT EDIT
