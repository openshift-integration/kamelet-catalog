// THIS FILE IS AUTOMATICALLY GENERATED: DO NOT EDIT

= image:kamelets/aws-ddb-streams-source.svg[] AWS DynamoDB Streams Source

*Provided by: "Red Hat"*

Receive events from Amazon DynamoDB Streams.
The basic authentication method for the AWS DynamoDB Streams service is to specify an access key and a secret key. These parameters are optional because the Kamelet provides a default credentials provider.
If you use the default credentials provider, the DynamoDB Streams client loads the credentials through this provider and doesn't use the basic authentication method.

== Configuration Options

The following table summarizes the configuration options available for the `aws-ddb-streams-source` Kamelet:
[width="100%",cols="2,^2,3,^2,^2,^3",options="header"]
|===
| Property| Name| Description| Type| Default| Example
| *region {empty}* *| AWS Region| The AWS region to access.| string| | `"eu-west-1"`
| *table {empty}* *| Table| The name of the DynamoDB table.| string| | 
| accessKey| Access Key| The access key obtained from AWS.| string| | 
| delay| Delay| The number of milliseconds before the next poll from the database.| integer| `500`| 
| overrideEndpoint| Endpoint Overwrite| Select this option to override the endpoint URI. To use this option, you must also provide a URI for the `uriEndpointOverride` option.| boolean| `false`| 
| secretKey| Secret Key| The secret key obtained from AWS.| string| | 
| streamIteratorType| Stream Iterator Type| Defines where in the DynamoDB stream to start getting records. There are two enums and the value can be one of FROM_LATEST and FROM_START. Note that using FROM_START can cause a significant delay before the stream has caught up to real-time.| string| `"FROM_LATEST"`| 
| uriEndpointOverride| Overwrite Endpoint URI| The overriding endpoint URI. To use this option, you must also select the `overrideEndpoint` option.| string| | 
| useDefaultCredentialsProvider| Default Credentials Provider| If true, the DynamoDB client loads credentials through a default credentials provider. If false, it uses the basic authentication method (access key and secret key).| boolean| `false`| 
|===

NOTE: Fields marked with an asterisk ({empty}*) are mandatory.


== Dependencies

At runtime, the `aws-ddb-streams-source` Kamelet relies upon the presence of the following dependencies:

- camel:gson
- camel:aws2-ddb
- camel:kamelet 

== Usage

This section describes how you can use the `aws-ddb-streams-source`.

=== Knative Source

You can use the `aws-ddb-streams-source` Kamelet as a Knative source by binding it to a Knative object.

.aws-ddb-streams-source-binding.yaml
[source,yaml]
----
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: aws-ddb-streams-source-binding
spec:
  source:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1alpha1
      name: aws-ddb-streams-source
    properties:
      region: "eu-west-1"
      table: "The Table"
  sink:
    ref:
      kind: Channel
      apiVersion: messaging.knative.dev/v1
      name: mychannel
  
----

==== *Prerequisite*

Make sure you have *"Red Hat Integration - Camel K"* installed into the OpenShift cluster you're connected to.

==== *Procedure for using the cluster CLI*

. Save the `aws-ddb-streams-source-binding.yaml` file to your local drive, and then edit it as needed for your configuration.

. Run the source by using the following command:
+
[source,shell]
----
oc apply -f aws-ddb-streams-source-binding.yaml
----

==== *Procedure for using the Kamel CLI*

Configure and run the source by using the following command:

[source,shell]
----
kamel bind aws-ddb-streams-source -p "source.region=eu-west-1" -p "source.table=The Table" channel:mychannel
----

This command creates the KameletBinding in the current namespace on the cluster.

=== Kafka Source

You can use the `aws-ddb-streams-source` Kamelet as a Kafka source by binding it to a Kafka topic.

.aws-ddb-streams-source-binding.yaml
[source,yaml]
----
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: aws-ddb-streams-source-binding
spec:
  source:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1alpha1
      name: aws-ddb-streams-source
    properties:
      region: "eu-west-1"
      table: "The Table"
  sink:
    ref:
      kind: KafkaTopic
      apiVersion: kafka.strimzi.io/v1beta1
      name: my-topic
  
----

==== *Prerequisites*

Ensure that you've installed the *AMQ Streams* operator in your OpenShift cluster and created a topic named `my-topic` in the current namespace.
Make also sure you have *"Red Hat Integration - Camel K"* installed into the OpenShift cluster you're connected to.

==== *Procedure for using the cluster CLI*

. Save the `aws-ddb-streams-source-binding.yaml` file to your local drive, and then edit it as needed for your configuration.

. Run the source by using the following command:
+
[source,shell]
----
oc apply -f aws-ddb-streams-source-binding.yaml
----

==== *Procedure for using the Kamel CLI*

Configure and run the source by using the following command:

[source,shell]
----
kamel bind aws-ddb-streams-source -p "source.region=eu-west-1" -p "source.table=The Table" kafka.strimzi.io/v1beta1:KafkaTopic:my-topic
----

This command creates the KameletBinding in the current namespace on the cluster.

== Kamelet source file

https://github.com/openshift-integration/kamelet-catalog/blob/main/aws-ddb-streams-source.kamelet.yaml

// THIS FILE IS AUTOMATICALLY GENERATED: DO NOT EDIT
