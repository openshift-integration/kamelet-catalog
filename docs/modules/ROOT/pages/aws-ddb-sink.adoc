// THIS FILE IS AUTOMATICALLY GENERATED: DO NOT EDIT

= image:kamelets/aws-ddb-sink.svg[] AWS DynamoDB Sink

*Provided by: "Red Hat"*

Send data to AWS DynamoDB service. The sent data will insert/update/delete an item on the given AWS DynamoDB table.

Access Key/Secret Key are the basic method for authenticating to the AWS DynamoDB service. These parameters are optional, because the Kamelet provide also the 'useDefaultCredentialsProvider'.

When using a default Credentials Provider the AWS DynamoDB client will load the credentials through this provider and won't use the static credential. This is reason for not having the access key and secret key as mandatory parameter for this Kamelet.

This Kamelet expects a JSON as body. The mapping between the JSON fields and table attribute values is done by key, so if you have the input:

'{"username":"oscerd", "city":"Rome"}'

The Kamelet will insert/update an item in the given AWS DynamoDB table and set the attributes 'username' and 'city' respectively. Please note that the JSON object must include the primary key values that define the item.

== Configuration Options

The following table summarizes the configuration options available for the `aws-ddb-sink` Kamelet:
[width="100%",cols="2,^2,3,^2,^2,^3",options="header"]
|===
| Property| Name| Description| Type| Default| Example
| *region {empty}* *| AWS Region| The AWS region to connect to| string| | `"eu-west-1"`
| *table {empty}* *| Table| Name of the DynamoDB table to look at| string| | 
| accessKey| Access Key| The access key obtained from AWS| string| | 
| operation| Operation| The operation to perform (one of PutItem, UpdateItem, DeleteItem)| string| `"PutItem"`| `"PutItem"`
| overrideEndpoint| Endpoint Overwrite| Set the need for overiding the endpoint URI. This option needs to be used in combination with uriEndpointOverride setting.| boolean| `false`| 
| secretKey| Secret Key| The secret key obtained from AWS| string| | 
| uriEndpointOverride| Overwrite Endpoint URI| Set the overriding endpoint URI. This option needs to be used in combination with overrideEndpoint option.| string| | 
| useDefaultCredentialsProvider| Default Credentials Provider| Set whether the DynamoDB client should expect to load credentials through a default credentials provider or to expect static credentials to be passed in.| boolean| `false`| 
| writeCapacity| Write Capacity| The provisioned throughput to reserved for writing resources to your table| integer| `1`| 
|===

NOTE: Fields marked with an asterisk ({empty}*) are mandatory.


== Dependencies

At runtime, the `aws-ddb-sink` Kamelet relies upon the presence of the following dependencies:

- mvn:org.apache.camel.kamelets:camel-kamelets-utils:1.8.0
- camel:core
- camel:jackson
- camel:aws2-ddb
- camel:kamelet 

== Usage

This section describes how you can use the `aws-ddb-sink`.

=== Knative Sink

You can use the `aws-ddb-sink` Kamelet as a Knative sink by binding it to a Knative object.

.aws-ddb-sink-binding.yaml
[source,yaml]
----
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: aws-ddb-sink-binding
spec:
  source:
    ref:
      kind: Channel
      apiVersion: messaging.knative.dev/v1
      name: mychannel
  sink:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1alpha1
      name: aws-ddb-sink
    properties:
      region: "eu-west-1"
      table: "The Table"
  
----

==== *Prerequisite*

Make sure you have *"Red Hat Integration - Camel K"* installed into the OpenShift cluster you're connected to.

==== *Procedure for using the cluster CLI*

. Save the `aws-ddb-sink-binding.yaml` file to your local drive, and then edit it as needed for your configuration.

. Run the sink by using the following command:
+
[source,shell]
----
oc apply -f aws-ddb-sink-binding.yaml
----

==== *Procedure for using the Kamel CLI*

Configure and run the sink by using the following command:

[source,shell]
----
kamel bind channel:mychannel aws-ddb-sink -p "sink.region=eu-west-1" -p "sink.table=The Table"
----

This command creates the KameletBinding in the current namespace on the cluster.

=== Kafka Sink

You can use the `aws-ddb-sink` Kamelet as a Kafka sink by binding it to a Kafka topic.

.aws-ddb-sink-binding.yaml
[source,yaml]
----
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: aws-ddb-sink-binding
spec:
  source:
    ref:
      kind: KafkaTopic
      apiVersion: kafka.strimzi.io/v1beta1
      name: my-topic
  sink:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1alpha1
      name: aws-ddb-sink
    properties:
      region: "eu-west-1"
      table: "The Table"
  
----

==== *Prerequisites*

Ensure that you've installed the *AMQ Streams* operator in your OpenShift cluster and created a topic named `my-topic` in the current namespace.
Make also sure you have *"Red Hat Integration - Camel K"* installed into the OpenShift cluster you're connected to.

==== *Procedure for using the cluster CLI*

. Save the `aws-ddb-sink-binding.yaml` file to your local drive, and then edit it as needed for your configuration.

. Run the sink by using the following command:
+
[source,shell]
----
oc apply -f aws-ddb-sink-binding.yaml
----

==== *Procedure for using the Kamel CLI*

Configure and run the sink by using the following command:

[source,shell]
----
kamel bind kafka.strimzi.io/v1beta1:KafkaTopic:my-topic aws-ddb-sink -p "sink.region=eu-west-1" -p "sink.table=The Table"
----

This command creates the KameletBinding in the current namespace on the cluster.

== Kamelet source file

https://github.com/openshift-integration/kamelet-catalog/blob/main/aws-ddb-sink.kamelet.yaml

// THIS FILE IS AUTOMATICALLY GENERATED: DO NOT EDIT
