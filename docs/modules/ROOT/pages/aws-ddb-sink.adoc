// THIS FILE IS AUTOMATICALLY GENERATED: DO NOT EDIT

= image:kamelets/aws-ddb-sink.svg[] AWS DynamoDB Sink

*Provided by: "Red Hat"*

Send data to Amazon DynamoDB. The sent data inserts, updates, or deletes an item on the specified AWS DynamoDB table.

The basic authentication method for the AWS DynamoDB service is to specify an access key and a secret key. These parameters are optional because the Kamelet provides a default credentials provider.

If you use the default credentials provider, the DynamoDB client loads the credentials through this provider and doesn't use the basic authentication method.

This Kamelet expects a JSON-formatted body and it must include the primary key values that define the DynamoDB item. The mapping between the JSON fields and table attribute values is done by key. For example, for  '{"username":"oscerd", "city":"Rome"}' input, the Kamelet inserts or update an item in the specified AWS DynamoDB table and sets the values for the 'username' and 'city' attributes.

For PutItem operation the Json body defines all item attributes.
For DeleteItem operation the Json body defines only the primary key attributes that identify the item to delete.
For UpdateItem operation the Json body defines both key attributes to identify the item to be updated and all item attributes tht get updated on the item.

The given Json body can use "key" and "item" as top level properties. Both define a Json object that will be mapped to respective attribute value maps
{
    "key": {},
    "item": {}
}

== Configuration Options

The following table summarizes the configuration options available for the `aws-ddb-sink` Kamelet:
[width="100%",cols="2,^2,3,^2,^2,^3",options="header"]
|===
| Property| Name| Description| Type| Default| Example
| *region {empty}* *| AWS Region| The AWS region to access.| string| | 
| *table {empty}* *| Table| The name of the DynamoDB table.| string| | 
| accessKey| Access Key| The access key obtained from AWS.| string| | 
| operation| Operation| The operation to perform.| string| `"PutItem"`| `"PutItem"`
| overrideEndpoint| Endpoint Overwrite| Select this option to override the endpoint URI. To use this option, you must also provide a URI for the `uriEndpointOverride` option.| boolean| `false`| 
| secretKey| Secret Key| The secret key obtained from AWS.| string| | 
| uriEndpointOverride| Overwrite Endpoint URI| The overriding endpoint URI. To use this option, you must also select the `overrideEndpoint` option.| string| | 
| useDefaultCredentialsProvider| Default Credentials Provider| If true, the DynamoDB client loads credentials through a default credentials provider. If false, it uses the basic authentication method (access key and secret key).| boolean| `false`| 
|===

NOTE: Fields marked with an asterisk ({empty}*) are mandatory.


== Dependencies

At runtime, the `aws-ddb-sink` Kamelet relies upon the presence of the following dependencies:

- github:openshift-integration.kamelet-catalog:camel-kamelets-utils:2.2.0-SNAPSHOT
- camel:core
- camel:jackson
- camel:aws2-ddb
- camel:kamelet 

== Usage

This section describes how you can use the `aws-ddb-sink`.

=== Knative Sink

You can use the `aws-ddb-sink` Kamelet as a Knative sink by binding it to a Knative object.

.aws-ddb-sink-binding.yaml
[source,yaml]
----
apiVersion: camel.apache.org/v1
kind: KameletBinding
metadata:
  name: aws-ddb-sink-binding
spec:
  source:
    ref:
      kind: Channel
      apiVersion: messaging.knative.dev/v1
      name: mychannel
  sink:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: aws-ddb-sink
    properties:
      region: "The AWS Region"
      table: "The Table"
  
----

==== *Prerequisite*

Make sure you have *"Red Hat Integration - Camel K"* installed into the OpenShift cluster you're connected to.

==== *Procedure for using the cluster CLI*

. Save the `aws-ddb-sink-binding.yaml` file to your local drive, and then edit it as needed for your configuration.

. Run the sink by using the following command:
+
[source,shell]
----
oc apply -f aws-ddb-sink-binding.yaml
----

==== *Procedure for using the Kamel CLI*

Configure and run the sink by using the following command:

[source,shell]
----
kamel bind channel:mychannel aws-ddb-sink -p "sink.region=The AWS Region" -p "sink.table=The Table"
----

This command creates the KameletBinding in the current namespace on the cluster.

=== Kafka Sink

You can use the `aws-ddb-sink` Kamelet as a Kafka sink by binding it to a Kafka topic.

.aws-ddb-sink-binding.yaml
[source,yaml]
----
apiVersion: camel.apache.org/v1
kind: KameletBinding
metadata:
  name: aws-ddb-sink-binding
spec:
  source:
    ref:
      kind: KafkaTopic
      apiVersion: kafka.strimzi.io/v1beta1
      name: my-topic
  sink:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: aws-ddb-sink
    properties:
      region: "The AWS Region"
      table: "The Table"
  
----

==== *Prerequisites*

Ensure that you've installed the *AMQ Streams* operator in your OpenShift cluster and created a topic named `my-topic` in the current namespace.
Make also sure you have *"Red Hat Integration - Camel K"* installed into the OpenShift cluster you're connected to.

==== *Procedure for using the cluster CLI*

. Save the `aws-ddb-sink-binding.yaml` file to your local drive, and then edit it as needed for your configuration.

. Run the sink by using the following command:
+
[source,shell]
----
oc apply -f aws-ddb-sink-binding.yaml
----

==== *Procedure for using the Kamel CLI*

Configure and run the sink by using the following command:

[source,shell]
----
kamel bind kafka.strimzi.io/v1beta1:KafkaTopic:my-topic aws-ddb-sink -p "sink.region=The AWS Region" -p "sink.table=The Table"
----

This command creates the KameletBinding in the current namespace on the cluster.

== Kamelet source file

https://github.com/openshift-integration/kamelet-catalog/blob/main/aws-ddb-sink.kamelet.yaml

// THIS FILE IS AUTOMATICALLY GENERATED: DO NOT EDIT
