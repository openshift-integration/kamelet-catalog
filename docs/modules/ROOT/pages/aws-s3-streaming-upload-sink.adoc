// THIS FILE IS AUTOMATICALLY GENERATED: DO NOT EDIT

= image:kamelets/aws-s3-streaming-upload-sink.svg[] AWS S3 Streaming upload Sink

*Provided by: "Red Hat"*

Upload data to AWS S3 in streaming upload mode.

Access Key/Secret Key are the basic method for authenticating to the AWS S3 Service. These parameters are optional because the Kamelet provides the 'useDefaultCredentialsProvider'.

When using a default Credentials Provider the S3 client will load the credentials through this provider and won't use the static credential. This is reason for not having the access key and secret key as mandatory parameter for this Kamelet.

== Configuration Options

The following table summarizes the configuration options available for the `aws-s3-streaming-upload-sink` Kamelet:
[width="100%",cols="2,^2,3,^2,^2,^3",options="header"]
|===
| Property| Name| Description| Type| Default| Example
| *bucketNameOrArn {empty}* *| Bucket Name| The S3 Bucket name or Amazon Resource Name (ARN)..| string| | 
| *keyName {empty}* *| Key Name| Setting the key name for an element in the bucket through endpoint parameter. In Streaming Upload, with the default configuration, this will be the base for the progressive creation of files.| string| | 
| *region {empty}* *| AWS Region| The AWS region to access.| string| | 
| accessKey| Access Key| The access key obtained from AWS.| string| | 
| autoCreateBucket| Autocreate Bucket| Setting the autocreation of the S3 bucket bucketName.| boolean| `false`| 
| batchMessageNumber| Batch Message Number| The number of messages composing a batch in streaming upload mode| integer| `10`| 
| batchSize| Batch Size| The batch size (in bytes) in streaming upload mode| integer| `1000000`| 
| namingStrategy| Naming Strategy| The naming strategy to use in streaming upload mode. There are 2 enums and the value can be one of progressive, random| string| `"progressive"`| 
| overrideEndpoint| Endpoint Overwrite| Select this option to override the endpoint URI. To use this option, you must also provide a URI for the `uriEndpointOverride` option.| boolean| `false`| 
| restartingPolicy| Restarting Policy| The restarting policy to use in streaming upload mode. There are 2 enums and the value can be one of override, lastPart| string| `"lastPart"`| 
| secretKey| Secret Key| The secret key obtained from AWS.| string| | 
| streamingUploadTimeout| Streaming Upload Timeout| While streaming upload mode is true, this option set the timeout to complete upload| integer| | 
| uriEndpointOverride| Overwrite Endpoint URI| The overriding endpoint URI. To use this option, you must also select the `overrideEndpoint` option.| string| | 
| useDefaultCredentialsProvider| Default Credentials Provider| Set whether the S3 client should expect to load credentials through a default credentials provider or to expect static credentials to be passed in.| boolean| `false`| 
|===

NOTE: Fields marked with an asterisk ({empty}*) are mandatory.


== Dependencies

At runtime, the `aws-s3-streaming-upload-sink` Kamelet relies upon the presence of the following dependencies:

- camel:aws2-s3
- camel:kamelet 

== Usage

This section describes how you can use the `aws-s3-streaming-upload-sink`.

=== Knative Sink

You can use the `aws-s3-streaming-upload-sink` Kamelet as a Knative sink by binding it to a Knative object.

.aws-s3-streaming-upload-sink-binding.yaml
[source,yaml]
----
apiVersion: camel.apache.org/v1
kind: KameletBinding
metadata:
  name: aws-s3-streaming-upload-sink-binding
spec:
  source:
    ref:
      kind: Channel
      apiVersion: messaging.knative.dev/v1
      name: mychannel
  sink:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: aws-s3-streaming-upload-sink
    properties:
      bucketNameOrArn: "The Bucket Name"
      keyName: "The Key Name"
      region: "The AWS Region"
  
----

==== *Prerequisite*

Make sure you have *"Red Hat Integration - Camel K"* installed into the OpenShift cluster you're connected to.

==== *Procedure for using the cluster CLI*

. Save the `aws-s3-streaming-upload-sink-binding.yaml` file to your local drive, and then edit it as needed for your configuration.

. Run the sink by using the following command:
+
[source,shell]
----
oc apply -f aws-s3-streaming-upload-sink-binding.yaml
----

==== *Procedure for using the Kamel CLI*

Configure and run the sink by using the following command:

[source,shell]
----
kamel bind channel:mychannel aws-s3-streaming-upload-sink -p "sink.bucketNameOrArn=The Bucket Name" -p "sink.keyName=The Key Name" -p "sink.region=The AWS Region"
----

This command creates the KameletBinding in the current namespace on the cluster.

=== Kafka Sink

You can use the `aws-s3-streaming-upload-sink` Kamelet as a Kafka sink by binding it to a Kafka topic.

.aws-s3-streaming-upload-sink-binding.yaml
[source,yaml]
----
apiVersion: camel.apache.org/v1
kind: KameletBinding
metadata:
  name: aws-s3-streaming-upload-sink-binding
spec:
  source:
    ref:
      kind: KafkaTopic
      apiVersion: kafka.strimzi.io/v1beta1
      name: my-topic
  sink:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: aws-s3-streaming-upload-sink
    properties:
      bucketNameOrArn: "The Bucket Name"
      keyName: "The Key Name"
      region: "The AWS Region"
  
----

==== *Prerequisites*

Ensure that you've installed the *AMQ Streams* operator in your OpenShift cluster and created a topic named `my-topic` in the current namespace.
Make also sure you have *"Red Hat Integration - Camel K"* installed into the OpenShift cluster you're connected to.

==== *Procedure for using the cluster CLI*

. Save the `aws-s3-streaming-upload-sink-binding.yaml` file to your local drive, and then edit it as needed for your configuration.

. Run the sink by using the following command:
+
[source,shell]
----
oc apply -f aws-s3-streaming-upload-sink-binding.yaml
----

==== *Procedure for using the Kamel CLI*

Configure and run the sink by using the following command:

[source,shell]
----
kamel bind kafka.strimzi.io/v1beta1:KafkaTopic:my-topic aws-s3-streaming-upload-sink -p "sink.bucketNameOrArn=The Bucket Name" -p "sink.keyName=The Key Name" -p "sink.region=The AWS Region"
----

This command creates the KameletBinding in the current namespace on the cluster.

== Kamelet source file

https://github.com/openshift-integration/kamelet-catalog/blob/main/aws-s3-streaming-upload-sink.kamelet.yaml

// THIS FILE IS AUTOMATICALLY GENERATED: DO NOT EDIT
